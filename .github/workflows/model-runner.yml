name: Model Runner Deployment

on:
  push:
    branches: [ main ]
    paths:
      - 'deployment/Dockerfile.model-runner'
      - '.github/workflows/model-runner.yml'
  workflow_dispatch:

permissions:
  contents: read
  packages: write
  deployments: write

jobs:
  deploy-model-runner:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      - name: Build and push model runner image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: deployment/Dockerfile.model-runner
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/genz-model-runner:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Pull model
        env:
          DOCKER_MODEL_TOKEN: ${{ secrets.DOCKER_MODEL_TOKEN }}
        run: |
          docker model pull index.docker.io/ai/qwen2.5:7B-F16

      - name: Set up Dagger configuration
        run: |
          mkdir -p ~/.dagger
          echo "model_runner:
            base_url: http://model-runner.docker.internal/engines/llama.cpp/v1/
            disable_streaming: true
            default_model: index.docker.io/ai/qwen2.5:7B-F16" > ~/.dagger/config.yaml

      - name: Deploy with Dagger
        env:
          DAGGER_LOG_LEVEL: debug
          OPENAI_BASE_URL: http://model-runner.docker.internal/engines/llama.cpp/v1/
          OPENAI_DISABLE_STREAMING: true
          OPENAI_MODEL: index.docker.io/ai/qwen2.5:7B-F16
        run: |
          dagger project init
          dagger project update
          dagger do deploy
